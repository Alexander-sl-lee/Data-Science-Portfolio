{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Importing random trash\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pprint as pprint\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We are going to scrape indeed. It is not going to be fun\n",
    "\n",
    "**Lets test things out first**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://au.indeed.com/jobs?as_and=data+scientist&salary=$30,000+-+$100,000&limit=50&start=00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "pages = int(int(soup.find(\"div\", {\"id\":\"searchCount\"}).text[-3:])/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_indeedpage(page_soup, pay):\n",
    "    divs = soup.find_all(\"div\", class_ = [\"row\", \"result\"])    \n",
    "    for job_card in divs:\n",
    "        deets = {}\n",
    "\n",
    "        deets[\"id\"] = job_card.get(\"id\")\n",
    "\n",
    "        try:\n",
    "            deets[\"company\"] = job_card.find(\"span\", class_ = \"company\").text.strip(\"\\n\").strip()\n",
    "        except AttributeError:\n",
    "            deets[\"company\"] = None\n",
    "            \n",
    "        if pay == \"low\":\n",
    "            deets[\"pay\"] = 0\n",
    "        else:\n",
    "            deets[\"pay\"] = 1\n",
    "\n",
    "        deets[\"title\"] = job_card.a.get(\"title\")\n",
    "\n",
    "        deets[\"loc\"]= job_card.find(\"span\", class_ =\"location\").text\n",
    "\n",
    "        deets[\"summary\"] = job_card.find(\"span\", class_ = \"summary\").text.strip()\n",
    "        \n",
    "        yield deets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_jobs = []\n",
    "for job_listings in range(0,(pages+1)*50, 50):\n",
    "    r = requests.get(\"https://au.indeed.com/jobs?as_and=data+scientist&salary=$30,000+-+$100,000&limit=50&start={}\"\\\n",
    "                     .format(job_listings))\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    for i in scrape_indeedpage(soup, \"low\"):\n",
    "        all_jobs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for job_listings in range(0,(pages+1)*50, 50):\n",
    "    r = requests.get(\"https://au.indeed.com/jobs?as_and=data+scientist&salary=$100,000+-+$500,000&limit=50&start={}\"\\\n",
    "                     .format(job_listings))\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    for i in scrape_indeedpage(soup, \"high\"):\n",
    "        all_jobs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame.from_dict(all_jobs)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>id</th>\n",
       "      <th>loc</th>\n",
       "      <th>pay</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IESTEC PTY. LTD</td>\n",
       "      <td>p_0b114068555f1aad</td>\n",
       "      <td>Notting Hill VIC</td>\n",
       "      <td>0</td>\n",
       "      <td>SQL, Data Exploration tools, data modelling sk...</td>\n",
       "      <td>Internship - Engineering/Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Queensland Government</td>\n",
       "      <td>p_74e1e67bf37fdfe4</td>\n",
       "      <td>Brisbane QLD</td>\n",
       "      <td>0</td>\n",
       "      <td>This role will work as part of a team that man...</td>\n",
       "      <td>Statistical Data Standards Officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXPERIAN</td>\n",
       "      <td>p_9904565d2d67f9e3</td>\n",
       "      <td>Melbourne VIC</td>\n",
       "      <td>0</td>\n",
       "      <td>Successful candidates will enjoy working with ...</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Freelancer.com</td>\n",
       "      <td>p_a3ceb90d3e93ae9f</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>0</td>\n",
       "      <td>If this sounds like you, you will love life as...</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Youth &amp; Programmes Group</td>\n",
       "      <td>p_40a416d5f9ef6a1d</td>\n",
       "      <td>Canberra ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>To complete these investigations we use both t...</td>\n",
       "      <td>Data Analyst |Data Scientist - Canberra, ACT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    company                  id               loc  pay  \\\n",
       "0           IESTEC PTY. LTD  p_0b114068555f1aad  Notting Hill VIC    0   \n",
       "1     Queensland Government  p_74e1e67bf37fdfe4      Brisbane QLD    0   \n",
       "2                  EXPERIAN  p_9904565d2d67f9e3     Melbourne VIC    0   \n",
       "3            Freelancer.com  p_a3ceb90d3e93ae9f        Sydney NSW    0   \n",
       "4  Youth & Programmes Group  p_40a416d5f9ef6a1d      Canberra ACT    0   \n",
       "\n",
       "                                             summary  \\\n",
       "0  SQL, Data Exploration tools, data modelling sk...   \n",
       "1  This role will work as part of a team that man...   \n",
       "2  Successful candidates will enjoy working with ...   \n",
       "3  If this sounds like you, you will love life as...   \n",
       "4  To complete these investigations we use both t...   \n",
       "\n",
       "                                          title  \n",
       "0        Internship - Engineering/Data Analysis  \n",
       "1            Statistical Data Standards Officer  \n",
       "2                                Data Scientist  \n",
       "3                                Data Scientist  \n",
       "4  Data Analyst |Data Scientist - Canberra, ACT  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As we can see there are a few duplicates as defined by the job_id\n",
      "pj_83846c25c5a9cb38    4\n",
      "pj_aae852949bd35607    4\n",
      "pj_4906a89a2b6f19a3    3\n",
      "p_d01b71fc3b0b2ac4     3\n",
      "p_bfb4e0e59c376392     3\n",
      "Name: id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"As we can see there are a few duplicates as defined by the job_id\")\n",
    "print(data[\"id\"].value_counts()[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.drop_duplicates(data, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    156\n",
      "0    139\n",
      "Name: pay, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.pay.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_summary(summary):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the outputr is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(summary).get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    #\n",
    "    # 3. Conbert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    #\n",
    "    # 4.In Python, searching a set is much faster than searching a list, soconvert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    #\n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 6. Join the words back into one string seperated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-74f86a283c50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_summary\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_summary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   2156\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;31m# arg is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2158\u001b[1;33m             \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m         return self._constructor(new_values,\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-34049cce04f8>\u001b[0m in \u001b[0;36mclean_summary\u001b[1;34m(summary)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# 4.In Python, searching a set is much faster than searching a list, soconvert the stop words to a set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mstops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# 5. Remove stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "data[\"clean_summary\"] = data.summary.map(clean_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(data, \"indeed_datascientist.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Bag of Words model**\n",
    "Use CountVectorizer to create bag of words for:\n",
    "\n",
    "- Job titles\n",
    "- Job summarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting the vectorizer just like we would set a model\n",
    "cvec = CountVectorizer(stop_words = \"english\")\n",
    "\n",
    "#Fitting the vectorizer on our training data\n",
    "cvec.fit(data[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(cvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(cvec.transform(data['summary']).todense(),\n",
    "                      columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_counts = X_train.sum(axis=0)\n",
    "words_counts.sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',use_idf=True)\n",
    "model = vectorizer.fit_transform(data['summary'].str.upper())\n",
    "km = KMeans(n_clusters=5,init='k-means++',max_iter=200,n_init=1)\n",
    "\n",
    "k=km.fit(model)\n",
    "terms = vectorizer.get_feature_names()\n",
    "order_centroids = km.cluster_centers_.argsort()[:,::-1]\n",
    "for i in range(5):\n",
    "    print(\"cluster of words %d:\" %i)\n",
    "    for ind in order_centroids[i,:10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Create a predictor matrix of words from the quotes with CountVectorizer\n",
    "\n",
    "It is up to you what ngram range you want to select. **Make sure that `binary=True`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2), max_features=2500, binary=True, stop_words='english')\n",
    "words = cv.fit_transform(data.clean_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = pd.DataFrame(words.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Split data into training and testing splits\n",
    "\n",
    "You should keep 25% of the data in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(words.values, data.pay, test_size=0.25)\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Build a `BernoulliNB` model predicting high vs low salaries from the word appearances\n",
    "\n",
    "The model should only be built (and cross-validated) on the training data.\n",
    "\n",
    "Cross-validate the score and compare it to baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "nb.fit(Xtrain, ytrain)\n",
    "\n",
    "nb_scores = cross_val_score(BernoulliNB(), Xtrain, ytrain, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(nb_scores)\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"The average score of a BernoulliNB model is {:.2f}\".format(np.mean(nb_scores)))\n",
    "print(\"The baseline score is {:.2f}\".format(np.mean(np.mean(ytrain))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Pull out the probability of words given \"high salary\"\n",
    "\n",
    "The `.feature_log_prob_` attribute of the naive bayes model contains the log probabilities of a feature appearing given a target class.\n",
    "\n",
    "The rows correspond to the class of the target, and the columns correpsond to the features. The first row is the 0 \"low salary\" class, and the second is the 1 \"high salary\" class.\n",
    "\n",
    "#### 5.1 Pull out the log probabilities and convert them to probabilities (for high and low salaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_lp = nb.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_p = np.exp(feat_lp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_p = np.exp(feat_lp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Make a dataframe with the probabilities and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_probs = pd.DataFrame({'high_p':high_p, 'low_p':low_p, 'feature':words.columns.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_probs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Create a column that is the difference between fresh probability of appearance and rotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_probs['sal_diff'] = feat_probs.high_p - feat_probs.low_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Look at the most likely words for fresh and rotten reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_probs.sort_values('sal_diff', ascending=False, inplace=True)\n",
    "feat_probs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_probs.sort_values('sal_diff', ascending=True, inplace=True)\n",
    "feat_probs.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Examine how your model performs on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (nb.score(Xtest, ytest))\n",
    "print (np.mean(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
