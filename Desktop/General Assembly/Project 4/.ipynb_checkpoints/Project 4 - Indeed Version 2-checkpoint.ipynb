{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Project 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Importing modules**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pprint\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Scraping**\n",
    "\n",
    "Unfortunately since I'm not too familiar with scrapy and jupyter notebook, I dont know how to integrate the scrapy spider into the notebook. However, the below cell contains the code for spider scraper\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-51-f13e8b891917>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-51-f13e8b891917>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    -*- coding: utf-8 -*-\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import scrapy, re\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class IndeedSpider(CrawlSpider):\n",
    "    name = 'indeed'\n",
    "    allowed_domains = ['au.indeed.com']\n",
    "\n",
    "    pay = 60000\n",
    "\n",
    "    urls = ['https://au.indeed.com/jobs?q=data+scientist+%24{}+-+%24{}'.format(x-10000, x) for x in range(50000,210000,10000)]\n",
    "\n",
    "    start_urls = urls\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(\"start\\=.*$\"), callback='parse_item', follow = True),\n",
    "    )    \n",
    "\n",
    "    def parse_item(self, response):\n",
    "        self.log('------------------------------------')\n",
    "        self.log('I just visited: ' + response.url)\n",
    "        self.log('------------------------------------')\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        divs = soup.find_all(\"div\", class_ = [\"row\", \"result\"])\n",
    "\n",
    "\n",
    "        for job in divs:\n",
    "            \n",
    "            item = {}\n",
    "\n",
    "            item[\"id\"] = job.get(\"id\")\n",
    "\n",
    "            try:\n",
    "                item[\"company\"] = job.find(\"span\", class_ = \"company\").text.strip(\"\\n\").strip()\n",
    "            except AttributeError:\n",
    "                item[\"company\"] = None\n",
    "                \n",
    "            item[\"pay\"] = re.search('([0-9]*)&start', response.url).group(1)\n",
    "\n",
    "            item[\"title\"] = job.a.get(\"title\")\n",
    "\n",
    "            item[\"loc\"]= job.find(\"span\", class_ =\"location\").text\n",
    "\n",
    "            job_summ_page = job.get(\"data-jk\")\n",
    "            next_page_url = \"https://au.indeed.com/viewjob?jk={}&from=tp-serp&tk=1c2i82l0a102q7vk\".format(job_summ_page)\n",
    "            yield scrapy.Request(url = next_page_url, callback = self.parse_summ, meta=dict(item=item))\n",
    "\n",
    "    def parse_summ(self, response):\n",
    "        \n",
    "        item = response.meta['item']\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        item[\"summ\"] = soup.find(\"span\", class_ = \"summary\").text.replace(\"\\n\", \" \")\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Data**\n",
    "\n",
    "For the purpose of this project, I will be working on data that has already been scraped into a .json file from the spider. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initiates file name: file_name\n",
    "file_name = \"test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>id</th>\n",
       "      <th>loc</th>\n",
       "      <th>pay</th>\n",
       "      <th>summ</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Opus Recruitment Solutions</td>\n",
       "      <td>p_d4996fe3bbcdf3f3</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>120000</td>\n",
       "      <td>My client is a rapidly growing start-up within...</td>\n",
       "      <td>DATA SCIENTIST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QBE Insurance</td>\n",
       "      <td>pj_112015665e5a2e01</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>120000</td>\n",
       "      <td>QBE is one of the top 20 global general insure...</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xpand</td>\n",
       "      <td>p_95299eb857a58426</td>\n",
       "      <td>Australia</td>\n",
       "      <td>120000</td>\n",
       "      <td>Senior Data Scientist   An exponentially growi...</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      company                   id         loc     pay  \\\n",
       "0  Opus Recruitment Solutions   p_d4996fe3bbcdf3f3  Sydney NSW  120000   \n",
       "1               QBE Insurance  pj_112015665e5a2e01  Sydney NSW  120000   \n",
       "2                       Xpand   p_95299eb857a58426   Australia  120000   \n",
       "\n",
       "                                                summ                  title  \n",
       "0  My client is a rapidly growing start-up within...         DATA SCIENTIST  \n",
       "1  QBE is one of the top 20 global general insure...  Senior Data Scientist  \n",
       "2  Senior Data Scientist   An exponentially growi...  Senior Data Scientist  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports file as dataframe: data\n",
    "data = pd.read_json(file_name)\n",
    "\n",
    "# Displays the head of the dataframe\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 6)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company    object\n",
       "id         object\n",
       "loc        object\n",
       "pay         int64\n",
       "summ       object\n",
       "title      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company    0\n",
       "id         0\n",
       "loc        0\n",
       "pay        0\n",
       "summ       0\n",
       "title      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000    59\n",
       "110000    55\n",
       "90000     32\n",
       "120000    23\n",
       "130000    21\n",
       "80000      6\n",
       "70000      3\n",
       "150000     1\n",
       "Name: pay, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Distribution of pay scale for jobs\n",
    "data.pay.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Cleaning Job Titles and Job Summaries**\n",
    "\n",
    "Do to the inconsistencies with the job titles and job summaries, we are going to clean it up a bit. This involves removing all non-alphanumeric characters and making everything lowercase. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Regex to clean\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the outputr is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(text).get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    #\n",
    "    # 3. Conbert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    #\n",
    "    # 4.In Python, searching a set is much faster than searching a list, soconvert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    #\n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 6. Join the words back into one string seperated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.summ.map(clean_text);\n",
    "data.title.map(clean_text);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Bag of Words model**\n",
    "\n",
    "Use CountVectorizer to create bag of words for:\n",
    "\n",
    "- Job titles\n",
    "- Job summarys\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Setting the vectorizer just like we would set a model\n",
    "cvec = CountVectorizer(stop_words = \"english\")\n",
    "\n",
    "#Fitting the vectorizer on our training data\n",
    "cvec.fit(data[\"summ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(cvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(cvec.transform(data['summ']).todense(),\n",
    "                      columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 5893)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data           897\n",
       "experience     624\n",
       "team           459\n",
       "work           414\n",
       "role           359\n",
       "skills         332\n",
       "business       313\n",
       "research       304\n",
       "working        261\n",
       "development    250\n",
       "management     224\n",
       "science        204\n",
       "analytics      202\n",
       "new            201\n",
       "ability        199\n",
       "strong         188\n",
       "analysis       186\n",
       "apply          186\n",
       "australia      185\n",
       "projects       176\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_counts = X_train.sum(axis=0)\n",
    "words_counts.sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster of words 0:\n",
      " data\n",
      " risk\n",
      " marketing\n",
      " experience\n",
      " team\n",
      " work\n",
      " financial\n",
      " macquarie\n",
      " business\n",
      " customer\n",
      "\n",
      "cluster of words 1:\n",
      " stack\n",
      " backend\n",
      " developer\n",
      " experience\n",
      " jvm\n",
      " team\n",
      " engineers\n",
      " aws\n",
      " kotlin\n",
      " js\n",
      "\n",
      "cluster of words 2:\n",
      " data\n",
      " analytics\n",
      " learning\n",
      " experience\n",
      " business\n",
      " machine\n",
      " science\n",
      " team\n",
      " big\n",
      " statistical\n",
      "\n",
      "cluster of words 3:\n",
      " environmental\n",
      " research\n",
      " role\n",
      " management\n",
      " project\n",
      " experience\n",
      " australia\n",
      " csiro\n",
      " position\n",
      " work\n",
      "\n",
      "cluster of words 4:\n",
      " cardiac\n",
      " clinic\n",
      " heart\n",
      " tasmania\n",
      " launceston\n",
      " echo\n",
      " care\n",
      " imaging\n",
      " service\n",
      " uq\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',use_idf=True)\n",
    "model = vectorizer.fit_transform(data['summ'].str.upper())\n",
    "km = KMeans(n_clusters=5,init='k-means++',max_iter=200,n_init=1)\n",
    "\n",
    "k=km.fit(model)\n",
    "terms = vectorizer.get_feature_names()\n",
    "order_centroids = km.cluster_centers_.argsort()[:,::-1]\n",
    "for i in range(5):\n",
    "    print(\"cluster of words %d:\" %i)\n",
    "    for ind in order_centroids[i,:10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Create a predictor matrix of words from the quotes with CountVectorizer\n",
    "\n",
    "It is up to you what ngram range you want to select. **Make sure that `binary=True`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2), max_features=2500, binary=True, stop_words='english')\n",
    "words = cv.fit_transform(data.summ)\n",
    "words = pd.DataFrame(words.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>103k</th>\n",
       "      <th>109k</th>\n",
       "      <th>109k 128k</th>\n",
       "      <th>11</th>\n",
       "      <th>11 59pm</th>\n",
       "      <th>...</th>\n",
       "      <th>www macquarie</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>years ago</th>\n",
       "      <th>years experience</th>\n",
       "      <th>years professional</th>\n",
       "      <th>years related</th>\n",
       "      <th>years relevant</th>\n",
       "      <th>york</th>\n",
       "      <th>zealand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  02  03  10  100  103k  109k  109k 128k  11  11 59pm   ...     \\\n",
       "0    0   0   0   0    0     0     0          0   0        0   ...      \n",
       "1    0   0   0   0    0     0     0          0   0        0   ...      \n",
       "2    0   0   1   0    0     0     0          0   0        0   ...      \n",
       "3    0   0   0   0    0     0     0          0   0        0   ...      \n",
       "4    1   0   0   0    0     0     0          0   0        0   ...      \n",
       "\n",
       "   www macquarie  year  years  years ago  years experience  \\\n",
       "0              0     1      0          0                 0   \n",
       "1              0     0      0          0                 0   \n",
       "2              0     0      0          0                 0   \n",
       "3              0     0      0          0                 0   \n",
       "4              0     0      1          0                 1   \n",
       "\n",
       "   years professional  years related  years relevant  york  zealand  \n",
       "0                   0              0               0     0        0  \n",
       "1                   0              0               0     0        0  \n",
       "2                   0              0               0     0        0  \n",
       "3                   0              0               0     0        0  \n",
       "4                   0              0               0     0        0  \n",
       "\n",
       "[5 rows x 2500 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2500)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Split pay into high - low.\n",
    "\n",
    "Lets split pay into high - low and see the differences in the words used between the two categories. We're going to split it by median which is ~ $11,0000\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-3c17041454db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pay\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pay\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   2156\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;31m# arg is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2158\u001b[1;33m             \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m         return self._constructor(new_values,\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not callable"
     ]
    }
   ],
   "source": [
    "data[\"pay\"].map(lambda x: 1 for x in data[\"pay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Split data into training and testing splits\n",
    "\n",
    "You should keep 25% of the data in the test set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2500) (50, 2500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(words.values, data.pay, test_size=0.25)\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Build a `BernoulliNB` model predicting high vs low salaries from the word appearances\n",
    "\n",
    "The model should only be built (and cross-validated) on the training data.\n",
    "\n",
    "Cross-validate the score and compare it to baseline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "nb.fit(Xtrain, ytrain)\n",
    "\n",
    "nb_scores = cross_val_score(BernoulliNB(), Xtrain, ytrain, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.42424242  0.375       0.4         0.17857143  0.18518519]\n",
      "----------------------------------------------------------------------\n",
      "The average score of a BernoulliNB model is 0.31\n",
      "The baseline score is 106200.00\n"
     ]
    }
   ],
   "source": [
    "print(nb_scores)\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"The average score of a BernoulliNB model is {:.2f}\".format(np.mean(nb_scores)))\n",
    "print(\"The baseline score is {:.2f}\".format(np.mean(np.mean(ytrain))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
